{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to find the best porforming classification models. We will be training the models to determine the if any particular page on a PDF is a map (aka alignment sheet) or not. \n",
    "\n",
    "\n",
    "In this code we will Once the necessary libraries are imported, the following actions are performed:\n",
    "\n",
    "- <strong>Load labelled data: </strong>\n",
    "Here we generate the features using \"extract_features\" function.  \n",
    "\n",
    "- <strong>Train test split: </strong>\n",
    "Split the dataset into test and train set. \n",
    "\n",
    "- <strong>Prepare validation set: </strong>\n",
    "Create validation dataframe.\n",
    "\n",
    "- <strong>Implement classification models: </strong>\n",
    "Train various classification models and then get accuracy score and confusion matric for test and validation set.  \n",
    "\n",
    "- <strong>Compare models: </strong>\n",
    "Compare the accuracy score and the confusion matrix and save the best model for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction import extract_features\n",
    "\n",
    "path = os.getcwd()\n",
    "path = os.path.abspath('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labeled data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the PDF names (or Data IDs) and we also have manually marked which pages on these PDFs are maps. First, we create the dataframe of the features by extracting the features of each page on these PDFs using the function \"extract_features\". Then by using the marked values of each page being map or not we create the dataframe for the dependent variable. The variable \"dataID_pageNo\" is onnly used to identify a certain page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "path_pdf = path + \"\\\\TrainingSet\\\\\"\n",
    "\n",
    "DataIDHand = [268712,  486221, 500633, \n",
    "               555093, 684494, \n",
    "              895015, 2392922, 2445549, 2758927,\n",
    "              2813701,  \n",
    "              2967854, 2968069,  \n",
    "              3891802,\n",
    "              4036098]\n",
    "Pages = [[3,4,5,8,9,10,14,15,24,25,26], range(1,5),  [5,9], \n",
    "          [6,9,33,34], [12,13,14],\n",
    "         range(1,11), [1], range(1,4),  [9],\n",
    "         [40, 92, 95, 143, 170, 180, 216, 217, 218, 219],\n",
    "         [3,4],range(1,13),  \n",
    "         [33, 34, 35, 89, 90, 91, 92, 93, 100, 146, 147, 148, 149, 153, 154, 159, 160, 161, 162, 165, 166, 169, 170, 173, 174, 177, 178, 181, 182, 184, 185, 188, 189], \n",
    "          []]\n",
    "\n",
    "print(len(DataIDHand))\n",
    "print(len(Pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Starting: 268712. PDF 1 out of 14\n",
      "File Starting: 486221. PDF 2 out of 14\n",
      "File Starting: 500633. PDF 3 out of 14\n"
     ]
    }
   ],
   "source": [
    "#fetching featuresfor the pages of the PDF Files\n",
    "X_df, dataIDs, error_files = extract_features(DataIDHand, path_pdf) \n",
    "#Features\n",
    "#dataIDs\n",
    "#error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.to_csv(path + \"\\\\root\\\\features_test_train.csv\")\n",
    "dataIDs.to_csv(path + \"\\\\root\\\\dataIDs.csv\")\n",
    "print(len(error_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.read_csv((path + \"\\\\root\\\\features_test_train.csv\", index_col = 0)\n",
    "dataIDs = pd.read_csv((path + \"\\\\root\\\\dataIDs.csv\", index_col = 0)\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_features = X_df.copy()\n",
    "X_df_features.drop(columns=['dataID_pageNo'], inplace=True)\n",
    "X_df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Y_values(dataIDs, Pages):\n",
    "    Y_class = []\n",
    "    dataID_pageNo = []\n",
    "    j = 0\n",
    "    for index, row in dataIDs.iterrows():\n",
    "        #print(row['DataIDs'])\n",
    "        #print(row['Page_no'])\n",
    "        for i in range(1,row['Page_no']+1):\n",
    "            if i in Pages[j]:\n",
    "                Y_class.append(1)\n",
    "            else:\n",
    "                Y_class.append(0)\n",
    "            dataID_pageNo.append(str(row['DataIDs']) + \"_\" +str(i))\n",
    "        j = j+1\n",
    "    \n",
    "    Y_df = pd.DataFrame({'dataID_pageNo' : dataID_pageNo, \n",
    "                         'Y_class' : Y_class})\n",
    "    Y_dfclass = pd.DataFrame({'Y_class' : Y_class})\n",
    "    \n",
    "    return Y_df, Y_dfclass\n",
    "    \n",
    "                \n",
    "Y_df, Y_dfclass = get_Y_values(dataIDs, Pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Y_df))\n",
    "print(len(X_df))\n",
    "print(len(Y_dfclass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the seed value to get the same results when rerunning the code. Then we split the dataset randomly into train set and test set. (train set = 0.75, test set =0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(19)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df_features,\n",
    "                                                    Y_dfclass,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 8)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Set: \", len(y_train))\n",
    "print(\"Alignment Sheets in Training Set: \", len(y_train[y_train.Y_class > 0]))\n",
    "print()\n",
    "print(\"Test Set: \", len(y_test))\n",
    "print(\"Alignment Sheets in Training Set: \", len(y_test[y_test.Y_class > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the validation set we make use of new PDFs representing a new real world problem. For validation sets we would except slighly lower level of accuracy. The model which performs better on validation set typically performs better overall. \n",
    "\n",
    "We extract features using 'extract_features' function as earlier. Then we create validation dataframes for features and dependent variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataIDHand = [3410189, 3970828, 2968356]\n",
    "Pages = [[], \n",
    "         [29, 35, 51, 59, 100, 101, 108, 109, 165, 179, 225, 231, 293, 294], \n",
    "         [9,18, 26]]\n",
    "\n",
    "print(len(DataIDHand))\n",
    "print(len(Pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pdf = (path + \"\\\\RF Validation Set\\\\\"\n",
    "\n",
    "# #fetching featuresfor the pages of the PDF Files\n",
    "X_df_valid, dataIDs_valid, error_files = extract_features(DataIDHand, path_pdf) \n",
    "# #Features\n",
    "# #dataIDs\n",
    "# #error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_valid.to_csv((path + \"\\\\root\\\\features_valid.csv\")\n",
    "dataIDs_valid.to_csv(path + \"\\\\root\\\\dataIDs_valid.csv\")\n",
    "print(len(error_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_valid = pd.read_csv((path + \"\\\\root\\\\features_valid.csv\", index_col = 0)\n",
    "dataIDs_valid = pd.read_csv((path + \"\\\\root\\\\dataIDs_valid.csv\", index_col = 0)\n",
    "X_df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_features_valid = X_df_valid.copy()\n",
    "X_df_features_valid.drop(columns=['dataID_pageNo'], inplace=True)\n",
    "X_df_features_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df_valid, Y_dfclass_valid = get_Y_values(dataIDs_valid, Pages)\n",
    "\n",
    "print(len(Y_df_valid))\n",
    "print(len(X_df_features_valid))\n",
    "print(len(X_df_valid))\n",
    "print(len(Y_dfclass_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are using regression models as classification models, hence they are collectively reffered to as classification models. <br>\n",
    "\n",
    "First we save the classification models and their names in an array. Then for each of these models first we fit the model using the training dataset and then generate the confusion matrix and accuracy score for each of these models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "name = []\n",
    "# we will create an array of Classifiers and append different classification models to our array.\n",
    "model1 = xgboost.XGBClassifier()\n",
    "classifiers.append(model1)\n",
    "name.append(\"xgboost\")\n",
    "\n",
    "model2 = svm.SVC()\n",
    "classifiers.append(model2)\n",
    "name.append(\"svc\")\n",
    "\n",
    "model3 = tree.DecisionTreeClassifier()\n",
    "classifiers.append(model3)\n",
    "name.append(\"decisiontree\")\n",
    "\n",
    "model4 = RandomForestClassifier()\n",
    "classifiers.append(model4)\n",
    "name.append(\"rfc\")\n",
    "\n",
    "\n",
    "model5 = RandomForestRegressor(n_estimators=5)\n",
    "classifiers.append(model5)\n",
    "name.append(\"rfr5\")\n",
    "\n",
    "model6 = RandomForestRegressor(n_estimators=25)\n",
    "classifiers.append(model6)\n",
    "name.append(\"rfr25\")\n",
    "\n",
    "model7 = RandomForestRegressor(n_estimators=50)\n",
    "classifiers.append(model7)\n",
    "name.append(\"rfr50\")\n",
    "\n",
    "model8 = RandomForestRegressor(n_estimators=75)\n",
    "classifiers.append(model8)\n",
    "name.append(\"rfr75\")\n",
    "\n",
    "model9 = RandomForestRegressor(n_estimators=100)\n",
    "classifiers.append(model9)\n",
    "name.append(\"rfr100\")\n",
    "\n",
    "\n",
    "model10 = XGBRegressor(n_estimators=5)\n",
    "classifiers.append(model10)\n",
    "name.append(\"xgbr5\")\n",
    "\n",
    "model11 = XGBRegressor(n_estimators=25)\n",
    "classifiers.append(model11)\n",
    "name.append(\"xgbr25\")\n",
    "\n",
    "model12 = XGBRegressor(n_estimators=50)\n",
    "classifiers.append(model12)\n",
    "name.append(\"xgbr50\")\n",
    "\n",
    "model13 = XGBRegressor(n_estimators=75)\n",
    "classifiers.append(model13)\n",
    "name.append(\"xgbr75\")\n",
    "\n",
    "model14 = XGBRegressor(n_estimators=100)\n",
    "classifiers.append(model14)\n",
    "name.append(\"xgbr100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "random.seed(10)\n",
    "test_accuracy = []\n",
    "valid_accuracy = []\n",
    "cm_test = []\n",
    "cm_valid = []\n",
    "for clf in classifiers:\n",
    "    print(\"________________________________________________________\")\n",
    "    print(\"________________________________________________________\")\n",
    "    #fit our algorithms in our Train dataset \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #get test dataset prediction\n",
    "    if \"rfr\" or \"xgbr\" in name[i]:\n",
    "        y_pred_nb = clf.predict(X_test)\n",
    "        #y_pred.shape\n",
    "        #y_pred\n",
    "        y_pred = []\n",
    "        for y in y_pred_nb:\n",
    "            if y > 0.50:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "    else:\n",
    "        y_pred= clf.predict(X_test)\n",
    "        \n",
    "    print(name[i])\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    test_accuracy.append(acc)\n",
    "    print(\"Accuracy of %s is %s\"%(clf, acc))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix of %s is %s\"%(clf, cm))\n",
    "    cm_test.append(cm)\n",
    "    \n",
    "    \n",
    "    print(\"________________Validation Set ___________________________\")\n",
    "    #get validation accuracy\n",
    "    if \"rfr\" or \"xgbr\" in name[i]:\n",
    "        y_pred_nb = clf.predict(X_df_features_valid)\n",
    "        #y_pred.shape\n",
    "        #y_pred\n",
    "        y_pred = []\n",
    "        for y in y_pred_nb:\n",
    "            if y > 0.50:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "    else:\n",
    "        y_pred= clf.predict(X_df_features_valid)\n",
    "        \n",
    "    print(name[i])\n",
    "    acc = accuracy_score(Y_dfclass_valid[\"Y_class\"], y_pred)\n",
    "    valid_accuracy.append(acc)\n",
    "    print(\"Accuracy of %s is %s\"%(clf, acc))\n",
    "    cm = confusion_matrix(Y_dfclass_valid[\"Y_class\"], y_pred)\n",
    "    print(\"Confusion Matrix of %s is %s\"%(clf, cm))\n",
    "    cm_valid.append(cm)\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we save the accuracy score and the confusion matric for the test and validation sets. Then we save the pickled version of the best classification model for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_models = pd.DataFrame({'name': name, \n",
    "                                     'test_accuracy': test_accuracy,\n",
    "                                     'test_cm': cm_test, \n",
    "                                     'valid_accuracy':valid_accuracy,\n",
    "                                     'valid_cm': cm_valid})\n",
    "classification_models[\"product\"] = classification_models[\"test_accuracy\"]*classification_models[\"valid_accuracy\"]\n",
    "\n",
    "classification_models = classification_models.sort_values(by=['product'])\n",
    "classification_models.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "random.seed(10)\n",
    "\n",
    "for clf in classifiers:\n",
    "    \n",
    "    if name[i] != \"rfc\":\n",
    "        i = i +1\n",
    "        continue\n",
    "    print(name[i])\n",
    "    clf.fit(X_train, y_train)\n",
    "    filename = path + \"\\\\root\\\\alignment_sheet_classifier_rfc.sav\"\n",
    "    pickle.dump(clf, open(filename, 'wb'))\n",
    "    \n",
    "    filename = \"alignment_sheet_classifier_rfr50.sav\"\n",
    "    pickle.dump(clf, open(filename, 'wb'))\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to observe the features and their importance score for classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_importance = clf.feature_importances_\n",
    "feature = []\n",
    "\n",
    "for col in X_df_features:\n",
    "    feature.append(col)\n",
    "    \n",
    "df_f_importance = pd.DataFrame({'Feature_Name' :  feature, \n",
    "                                'Importance':  f_importance})\n",
    "df_f_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
